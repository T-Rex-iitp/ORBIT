# RAG.py
import os
from openai import OpenAI
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings

# ================================
# 0. ê²½ë¡œ ì„¤ì • (ë„¤ í™˜ê²½ì— ë§ê²Œ ì¡°ì ˆ)
# ================================
DOC_DIR = "./AI-Enabled-IFTA/LLaMA-Factory/RAG_docs"          # RAGì— ì“¸ ë¬¸ì„œ í´ë”
DB_DIR = "./AI-Enabled-IFTA/LLaMA-Factory/RAG_DB"      # ë²¡í„°DB ì €ì¥ í´ë”

BASE_URL = "http://localhost:8000/v1"       # LLaMA-Factory API ì„œë²„ ì£¼ì†Œ
API_KEY = "EMPTY"                           # ì•„ë¬´ ë¬¸ìì—´ì´ë‚˜ OK

# ================================
# 1. LLaMA-Factory OpenAI ì„œë²„ ì„¤ì •
# ================================
# â””â”€â”€ ì´ ì „ì— ë‹¤ë¥¸ í„°ë¯¸ë„ì—ì„œ ë°˜ë“œì‹œ:
# API_PORT=8000 llamafactory-cli api \
#   --model_name_or_path ./weights/gpt-oss \
#   --template gpt \
#   --infer_backend huggingface \
#   --trust_remote_code true
# ë¥¼ ì‹¤í–‰í•´ë‘ì–´ì•¼ í•¨.

client = OpenAI(base_url=BASE_URL, api_key=API_KEY)

llm = ChatOpenAI(
    model="gpt-oss",    # ì„œë²„ìª½ì—ì„œ ì¸ì‹ìš© ì´ë¦„ (ê·¸ëƒ¥ ë¬¸ìì—´ì´ë©´ ë¨)
    base_url=BASE_URL,
    api_key=API_KEY,
)

# ================================
# 2. ë¬¸ì„œ ë¡œë”© & ìª¼ê°œê¸° & ë²¡í„°DB êµ¬ì¶•
# ================================
if not os.path.isdir(DOC_DIR):
    raise ValueError(f"ë¬¸ì„œ í´ë”ê°€ ì—†ìŒ: {DOC_DIR}")

print(f"[INFO] ë¬¸ì„œ í´ë”ì—ì„œ ë¡œë”© ì¤‘: {DOC_DIR}")
# txt íŒŒì¼ ê¸°ì¤€. í•„ìš”í•˜ë©´ glob="**/*.*" ë“±ìœ¼ë¡œ ë°”ê¿”ë„ ë¨.
loader = DirectoryLoader(DOC_DIR, glob="**/*.txt", show_progress=True)
docs = loader.load()

if not docs:
    print("[WARN] ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. DOC_DIR ì•ˆì— .txt íŒŒì¼ ë„£ì–´ì£¼ì„¸ìš”.")
else:
    print(f"[INFO] ë¡œë“œëœ ë¬¸ì„œ ê°œìˆ˜: {len(docs)}")

print("[INFO] ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ìª¼ê°œëŠ” ì¤‘...")
# ì—¬ê¸°ì„œ "ìª¼ê°œëŠ”" ë¶€ë¶„:
# - chunk_size=800: 800ì ì •ë„ë§ˆë‹¤ ì˜ë¼ì„œ
# - chunk_overlap=200: ì•ë’¤ë¡œ 200ìì”© ê²¹ì¹˜ê²Œ
splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,
    chunk_overlap=200,
)
splits = splitter.split_documents(docs)
print(f"[INFO] ë§Œë“¤ì–´ì§„ ì²­í¬ ê°œìˆ˜: {len(splits)}")

print("[INFO] ì„ë² ë”© & ë²¡í„° ìŠ¤í† ì–´(Chroma) ìƒì„± ì¤‘...")
os.makedirs(DB_DIR, exist_ok=True)

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

vectordb = Chroma.from_documents(
    documents=splits,
    embedding=embeddings,
    persist_directory=DB_DIR,
)

retriever = vectordb.as_retriever(search_kwargs={"k": 4})

# ================================
# 3. ì¸í„°ë™í‹°ë¸Œ RAG ì±„íŒ… ë£¨í”„
# ================================
print("\n=== RAG Chat ì‹œì‘! (ì¢…ë£Œ: /exit ë˜ëŠ” /quit ì…ë ¥) ===\n")

while True:
    try:
        user_query = input("ì§ˆë¬¸ > ").strip()
    except (EOFError, KeyboardInterrupt):
        print("\nì¢…ë£Œí•©ë‹ˆë‹¤.")
        break

    if user_query in ("/exit", "/quit"):
        print("ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break
    if not user_query:
        continue

    # ğŸ”¥ LangChain ìµœì‹  ë²„ì „: retriever.invoke() ì‚¬ìš©
    rel_docs = retriever.invoke(user_query)
    if not rel_docs:
        context = ""
    else:
        context = "\n\n---\n\n".join(d.page_content for d in rel_docs)

    system_prompt = (
        "ë„ˆëŠ” RAG ì–´ì‹œìŠ¤í„´íŠ¸ì•¼. ì•„ë˜ 'ì»¨í…ìŠ¤íŠ¸' ë‚´ìš©ì„ ìµœëŒ€í•œ í™œìš©í•´ì„œ "
        "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì¤˜. ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ëª¨ë¥¸ë‹¤ê³  ì†”ì§íˆ ë§í•´."
    )

    user_content = (
        f"[ì»¨í…ìŠ¤íŠ¸]\n{context}\n\n"
        f"[ì§ˆë¬¸]\n{user_query}"
    )

    # LLaMA-Factoryì˜ OpenAI í˜¸í™˜ /v1/chat/completions ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ
    response = client.chat.completions.create(
        model="gpt-oss",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_content},
        ],
    )

    answer = response.choices[0].message.content
    print("\n[ì „ì²´ë‹µë³€]")
    print(answer)
    answer_final = (lambda x: x.split("assistantfinal",1)[1].strip() if "assistantfinal" in x else x.strip())(response.choices[0].message.content)
    print("\n[ìµœì¢…ë‹µë³€]")
    print(answer_final)
    print("\n" + "=" * 60 + "\n")
