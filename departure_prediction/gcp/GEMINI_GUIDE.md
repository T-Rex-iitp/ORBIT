# GCP Gemini ì‚¬ìš© ê°€ì´ë“œ


## ì„¤ì • ë°©ë²•

### 1. GCP í”„ë¡œì íŠ¸ ì„¤ì •

```bash
# í”„ë¡œì íŠ¸ ìƒì„±
gcloud projects create YOUR_PROJECT_ID

# í”„ë¡œì íŠ¸ ì„¤ì •
gcloud config set project YOUR_PROJECT_ID

# Vertex AI API í™œì„±í™”
gcloud services enable aiplatform.googleapis.com
```

### 2. ì„œë¹„ìŠ¤ ê³„ì • ìƒì„±

```bash
# ì„œë¹„ìŠ¤ ê³„ì • ìƒì„±
gcloud iam service-accounts create gemini-client \
  --display-name="Gemini API Client"

# Vertex AI ì‚¬ìš© ê¶Œí•œ ë¶€ì—¬
gcloud projects add-iam-policy-binding YOUR_PROJECT_ID \
  --member=serviceAccount:gemini-client@YOUR_PROJECT_ID.iam.gserviceaccount.com \
  --role=roles/aiplatform.user

# í‚¤ ìƒì„±
gcloud iam service-accounts keys create gemini-key.json \
  --iam-account=gemini-client@YOUR_PROJECT_ID.iam.gserviceaccount.com
```

### 3. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
# í•„ìˆ˜
export GCP_PROJECT_ID=your-project-id
export GOOGLE_APPLICATION_CREDENTIALS=/path/to/gemini-key.json

# Gemini ì‚¬ìš© í™œì„±í™”
export USE_GEMINI=true

# ì˜µì…˜ (GCS ëª¨ë¸ë„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°)
export GCS_MODEL_BUCKET=your-model-bucket
export USE_GCS_MODEL=true
```

### 4. Python ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

```bash
pip install google-cloud-aiplatform pillow
```

---

## ì‚¬ìš© ë°©ë²•

### Python ì½”ë“œì—ì„œ

```python
from hybrid_predictor import HybridDeparturePredictor

# Gemini ì‚¬ìš©
predictor = HybridDeparturePredictor(
    model_path='models/delay_predictor_full.pkl',
    use_gemini=True,
    gemini_project_id='your-project-id'
)

# ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©
import os
os.environ['USE_GEMINI'] = 'true'
os.environ['GCP_PROJECT_ID'] = 'your-project-id'

predictor = HybridDeparturePredictor()
```

### í‹°ì¼“ OCR (Vision)

```python
from utils.gemini_client import GeminiTicketOCR

# ì´ë¯¸ì§€ì—ì„œ í‹°ì¼“ ì •ë³´ ì¶”ì¶œ
ocr = GeminiTicketOCR(project_id='your-project-id')
ticket_info = ocr.extract_with_vision('ticket.png')

print(ticket_info)
# {
#   'flight_number': 'B6123',
#   'departure_airport': 'JFK',
#   'arrival_airport': 'LAX',
#   'departure_time': '2026-02-10 14:30',
#   ...
# }
```

### ì¶œë°œ ì‹œê°„ ì¶”ì²œ (LLM)

```python
# ìë™ìœ¼ë¡œ Gemini ì‚¬ìš© (USE_GEMINI=trueì¸ ê²½ìš°)
result = predictor.recommend_departure(
    address="Times Square, New York",
    flight_info={...},
    travel_mode='TRANSIT'
)

print(result['recommendation'])
# "Based on your flight from JFK to LAX departing at 2:30 PM,
#  I recommend leaving Times Square by 10:45 AM..."
```

---

## ë¹„ìš© ê³„ì‚°

### Gemini 1.5 Flash (ì¶”ì²œ)

| ì‘ì—… | ìš”ì²­ ìˆ˜ | í† í° | ë¹„ìš© |
|------|--------|------|------|
| í‹°ì¼“ OCR (Vision) | 1íšŒ | 1K | $0.001 |
| ì¶œë°œ ì¶”ì²œ (LLM) | 1íšŒ | 2K | $0.002 |
| **í•©ê³„ (1íšŒ ì‚¬ìš©)** | | | **$0.003** |

### ì›”ê°„ ë¹„ìš© ì˜ˆì‹œ

**100ëª…/ì¼ ì‚¬ìš© ì‹œ:**
- ì¼ì¼: 100íšŒ Ã— $0.003 = $0.3
- ì›”ê°„: $0.3 Ã— 30 = **$9/ì›”** âœ…

**vs Ollama GPU ì„œë²„:** $100/ì›” âŒ

**ì ˆê°ì•¡:** $91/ì›” (90% ì ˆê°!) ğŸ’°

### Gemini Pro (ë” ì •í™•)

| ì‘ì—… | ë¹„ìš© |
|------|------|
| Vision | $0.0025/1K |
| LLM | $0.005/1K |
| **í•©ê³„** | **$0.0075/íšŒ** |

ì›” 3000íšŒ ì‚¬ìš© ì‹œ: **$22.5/ì›”** (ì—¬ì „íˆ ì €ë ´)

---

## ì„±ëŠ¥ ë¹„êµ

### ì†ë„

```
í‹°ì¼“ OCR:
- Ollama: 8-12ì´ˆ
- Gemini: 1-2ì´ˆ âš¡ (6ë°° ë¹ ë¦„)

LLM ì¶”ì²œ:
- Ollama: 5-10ì´ˆ
- Gemini: 1-2ì´ˆ âš¡ (5ë°° ë¹ ë¦„)
```

### ì •í™•ë„

```
í‹°ì¼“ ì •ë³´ ì¶”ì¶œ:
- Ollama: 75-80%
- Gemini: 95%+ âœ…

ìì—°ì–´ ìƒì„±:
- Ollama: 80%
- Gemini: 95%+ âœ…
```

---

## ëª¨ë²” ì‚¬ë¡€

### 1. í™˜ê²½ë³„ ì„¤ì •

```python
# config.py
import os

# ê°œë°œ: Ollama (ë¡œì»¬ í…ŒìŠ¤íŠ¸)
# í”„ë¡œë•ì…˜: Gemini (ë¹ ë¥´ê³  ì•ˆì •ì )
USE_GEMINI = os.getenv('ENVIRONMENT') == 'production'
```

### 2. Fallback ì „ëµ

```python
# Gemini ì‹¤íŒ¨ ì‹œ Ollamaë¡œ í´ë°±
try:
    if use_gemini:
        result = gemini_client.generate_text(prompt)
    else:
        result = ollama_generate(prompt)
except Exception as e:
    # ë‘˜ ë‹¤ ì‹¤íŒ¨ ì‹œ í…œí”Œë¦¿ ì‚¬ìš©
    result = fallback_template(data)
```

### 3. ìºì‹±

```python
# ê°™ì€ ì´ë¯¸ì§€ ë°˜ë³µ ë¶„ì„ ë°©ì§€
@lru_cache(maxsize=100)
def cached_ocr(image_hash):
    return gemini_client.analyze_image(image_path)
```

---

## ë¬¸ì œ í•´ê²°

### ê¶Œí•œ ì—ëŸ¬

```bash
# Vertex AI ê¶Œí•œ í™•ì¸
gcloud projects get-iam-policy YOUR_PROJECT_ID \
  --flatten="bindings[].members" \
  --filter="bindings.role:roles/aiplatform.user"
```

### API í™œì„±í™” ì—ëŸ¬

```bash
# API ìƒíƒœ í™•ì¸
gcloud services list --enabled --filter="aiplatform"

# í™œì„±í™”
gcloud services enable aiplatform.googleapis.com
```

### ë¹„ìš© ì´ˆê³¼ ë°©ì§€

```python
# ì¼ì¼ ì˜ˆì‚° ì„¤ì •
from google.cloud import billing

# ì˜ˆì‚° ì´ˆê³¼ ì‹œ ì•Œë¦¼
# GCP Console > Billing > Budgets & Alerts
```

---

## ë§ˆì´ê·¸ë ˆì´ì…˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] GCP í”„ë¡œì íŠ¸ ìƒì„±
- [ ] Vertex AI API í™œì„±í™”
- [ ] ì„œë¹„ìŠ¤ ê³„ì • ìƒì„± ë° í‚¤ ë‹¤ìš´ë¡œë“œ
- [ ] í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (GCP_PROJECT_ID, GOOGLE_APPLICATION_CREDENTIALS)
- [ ] `pip install google-cloud-aiplatform`
- [ ] `USE_GEMINI=true` ì„¤ì •
- [ ] ë¡œì»¬ í…ŒìŠ¤íŠ¸
- [ ] ë¹„ìš© ëª¨ë‹ˆí„°ë§ ì„¤ì •
- [ ] Ollama ì„œë²„ ì¢…ë£Œ (ë¹„ìš© ì ˆê°)

---

## ì¶”ê°€ ê¸°ëŠ¥

### 1. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

```python
# ì‹¤ì‹œê°„ ì‘ë‹µ (ì‚¬ìš©ì ê²½í—˜ ê°œì„ )
for chunk in gemini_client.generate_text_stream(prompt):
    print(chunk, end='', flush=True)
```

### 2. ë‹¤êµ­ì–´ ì§€ì›

```python
# í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ â†’ ì˜ì–´ ì‘ë‹µ
response = gemini_client.generate_text(
    "ì´ í•­ê³µê¶Œ ì •ë³´ë¥¼ ì˜ì–´ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
    language='en'
)
```

### 3. ë°°ì¹˜ ì²˜ë¦¬

```python
# ì—¬ëŸ¬ ì´ë¯¸ì§€ ë™ì‹œ ì²˜ë¦¬
images = ['ticket1.png', 'ticket2.png', 'ticket3.png']
results = gemini_client.batch_analyze(images)
```

---

## ê²°ë¡ 

âœ… **Gemini ì‚¬ìš© ê¶Œì¥ ì´ìœ :**
1. 90% ë¹„ìš© ì ˆê° ($100 â†’ $9/ì›”)
2. 6ë°° ë¹ ë¥¸ ì†ë„ (10ì´ˆ â†’ 2ì´ˆ)
3. 95% ë†’ì€ ì •í™•ë„
4. ì„œë²„ ê´€ë¦¬ ë¶ˆí•„ìš”
5. ë¬´í•œ í™•ì¥ ê°€ëŠ¥

OllamaëŠ” ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©, **í”„ë¡œë•ì…˜ì€ Gemini!** ğŸš€
